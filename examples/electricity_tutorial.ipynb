{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Fusion Transformer (TFT) Tutorial\n",
    "## Electricity Load Forecasting with ElectricityLoadDiagrams20112014 Dataset\n",
    "\n",
    "This notebook provides a comprehensive tutorial on using the **Temporal Fusion Transformer (TFT)** for multi-horizon time series forecasting. We'll use real-world electricity consumption data to demonstrate the complete workflow.\n",
    "\n",
    "### What is TFT?\n",
    "\n",
    "The Temporal Fusion Transformer is a state-of-the-art deep learning architecture designed for:\n",
    "- **Multi-horizon forecasting**: Predict multiple future time steps at once\n",
    "- **Interpretability**: Understand which features and time steps matter most\n",
    "- **Mixed inputs**: Handle static, known future, and observed-only features\n",
    "\n",
    "### Key Components\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    TFT Architecture                             │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│  Static Features ──► Variable Selection ──► Context Vectors    │\n",
    "│                                                    │            │\n",
    "│  Historical Data ──► Variable Selection ──► LSTM Encoder ─┐    │\n",
    "│                                                            │    │\n",
    "│  Future Known ────► Variable Selection ──► LSTM Decoder ──┼──► │\n",
    "│                                                            │    │\n",
    "│                              Interpretable Multi-Head ◄────┘    │\n",
    "│                                   Attention                     │\n",
    "│                                      │                          │\n",
    "│                              Quantile Outputs                   │\n",
    "│                          (Probabilistic Forecasts)              │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We'll use the **ElectricityLoadDiagrams20112014** dataset from UCI ML Repository:\n",
    "- 370 clients' electricity consumption\n",
    "- 15-minute intervals from 2011-2014\n",
    "- ~140,000 time steps per client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# TFT imports\n",
    "from tft.models import TemporalFusionTransformer\n",
    "from tft.data import create_dataloaders\n",
    "from tft.training import TFTTrainer, EarlyStopping, ModelCheckpoint\n",
    "from tft.utils import TFTConfig, get_device, print_device_info\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Check device\n",
    "print_device_info()\n",
    "device = get_device('auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and Load Data\n",
    "\n",
    "First, let's download the electricity dataset and explore its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00321/LD2011_2014.txt.zip\"\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_FILE = DATA_DIR / \"LD2011_2014.txt\"\n",
    "\n",
    "def download_data():\n",
    "    \"\"\"Download the electricity dataset.\"\"\"\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    zip_path = DATA_DIR / \"LD2011_2014.txt.zip\"\n",
    "    \n",
    "    if DATA_FILE.exists():\n",
    "        print(f\"Data already exists at {DATA_FILE}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Downloading from {DATA_URL}...\")\n",
    "    urllib.request.urlretrieve(DATA_URL, zip_path)\n",
    "    \n",
    "    print(\"Extracting...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR)\n",
    "    zip_path.unlink()\n",
    "    print(\"Done!\")\n",
    "\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data\n",
    "print(\"Loading data...\")\n",
    "df_raw = pd.read_csv(\n",
    "    DATA_FILE,\n",
    "    sep=';',\n",
    "    decimal=',',\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    ")\n",
    "\n",
    "print(f\"Raw data shape: {df_raw.shape}\")\n",
    "print(f\"Date range: {df_raw.index.min()} to {df_raw.index.max()}\")\n",
    "print(f\"Number of clients: {len(df_raw.columns)}\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration\n",
    "\n",
    "Let's visualize the electricity consumption patterns to understand our data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few clients for visualization\n",
    "sample_clients = df_raw.columns[:5].tolist()\n",
    "df_sample = df_raw[sample_clients]\n",
    "\n",
    "# Resample to daily for better visualization\n",
    "df_daily = df_sample.resample('1D').sum()\n",
    "\n",
    "# Plot time series\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Full time series\n",
    "ax1 = axes[0]\n",
    "for client in sample_clients:\n",
    "    ax1.plot(df_daily.index, df_daily[client], label=client, alpha=0.7)\n",
    "ax1.set_title('Daily Electricity Consumption (Full Period)', fontsize=14)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Consumption (kWh)')\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "# Zoom into 1 month\n",
    "ax2 = axes[1]\n",
    "df_month = df_sample['2012-06'].resample('1h').sum()\n",
    "for client in sample_clients[:3]:\n",
    "    ax2.plot(df_month.index, df_month[client], label=client, alpha=0.8)\n",
    "ax2.set_title('Hourly Consumption (June 2012)', fontsize=14)\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Consumption (kWh)')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze seasonality patterns\n",
    "client = sample_clients[0]\n",
    "df_hourly = df_raw[client].resample('1h').sum()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Hourly pattern (average by hour of day)\n",
    "hourly_pattern = df_hourly.groupby(df_hourly.index.hour).mean()\n",
    "axes[0].bar(hourly_pattern.index, hourly_pattern.values, color='steelblue', alpha=0.8)\n",
    "axes[0].set_title(f'Average Consumption by Hour\\n({client})', fontsize=12)\n",
    "axes[0].set_xlabel('Hour of Day')\n",
    "axes[0].set_ylabel('Avg Consumption')\n",
    "axes[0].set_xticks(range(0, 24, 3))\n",
    "\n",
    "# Daily pattern (average by day of week)\n",
    "daily_pattern = df_hourly.groupby(df_hourly.index.dayofweek).mean()\n",
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "axes[1].bar(days, daily_pattern.values, color='coral', alpha=0.8)\n",
    "axes[1].set_title(f'Average Consumption by Day of Week\\n({client})', fontsize=12)\n",
    "axes[1].set_xlabel('Day of Week')\n",
    "axes[1].set_ylabel('Avg Consumption')\n",
    "\n",
    "# Monthly pattern\n",
    "monthly_pattern = df_hourly.groupby(df_hourly.index.month).mean()\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "axes[2].bar(months, monthly_pattern.values, color='seagreen', alpha=0.8)\n",
    "axes[2].set_title(f'Average Consumption by Month\\n({client})', fontsize=12)\n",
    "axes[2].set_xlabel('Month')\n",
    "axes[2].set_ylabel('Avg Consumption')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Clear daily pattern: higher consumption during daytime\")\n",
    "print(\"- Weekly pattern: lower consumption on weekends\")\n",
    "print(\"- Seasonal pattern: varies by month (heating/cooling needs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Now let's prepare the data for TFT. We need to:\n",
    "1. Resample to a manageable frequency\n",
    "2. Convert to long format\n",
    "3. Add time-based features\n",
    "4. Split into train/val/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for this tutorial (reduced for faster training)\n",
    "NUM_CLIENTS = 3        # Number of clients to use\n",
    "RESAMPLE_FREQ = '4h'   # Resample frequency\n",
    "ENCODER_LENGTH = 42    # ~1 week at 4-hour intervals\n",
    "DECODER_LENGTH = 6     # ~24 hours ahead\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Number of clients: {NUM_CLIENTS}\")\n",
    "print(f\"  - Resample frequency: {RESAMPLE_FREQ}\")\n",
    "print(f\"  - Encoder length: {ENCODER_LENGTH} steps (~{ENCODER_LENGTH * 4} hours)\")\n",
    "print(f\"  - Decoder length: {DECODER_LENGTH} steps (~{DECODER_LENGTH * 4} hours)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df_raw, num_clients, resample_freq):\n",
    "    \"\"\"\n",
    "    Preprocess the electricity data for TFT.\n",
    "    \n",
    "    Steps:\n",
    "    1. Select subset of clients\n",
    "    2. Resample to target frequency\n",
    "    3. Convert to long format\n",
    "    4. Add time features\n",
    "    \"\"\"\n",
    "    # Select clients\n",
    "    selected_clients = df_raw.columns[:num_clients].tolist()\n",
    "    df = df_raw[selected_clients].copy()\n",
    "    \n",
    "    # Resample\n",
    "    df = df.resample(resample_freq).sum()\n",
    "    df = df.dropna()\n",
    "    df = df[(df != 0).any(axis=1)]\n",
    "    \n",
    "    print(f\"After resampling: {df.shape}\")\n",
    "    \n",
    "    # Convert to long format\n",
    "    df = df.reset_index()\n",
    "    df = df.melt(\n",
    "        id_vars=['index'],\n",
    "        var_name='client_id',\n",
    "        value_name='consumption',\n",
    "    )\n",
    "    df = df.rename(columns={'index': 'datetime'})\n",
    "    \n",
    "    # Create numeric client ID\n",
    "    client_mapping = {c: i for i, c in enumerate(selected_clients)}\n",
    "    df['client_id_num'] = df['client_id'].map(client_mapping)\n",
    "    \n",
    "    return df, selected_clients\n",
    "\n",
    "df, clients = preprocess_data(df_raw, NUM_CLIENTS, RESAMPLE_FREQ)\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n",
    "print(f\"Clients: {clients}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    \"\"\"\n",
    "    Add time-based features for the TFT model.\n",
    "    \n",
    "    TFT uses three types of features:\n",
    "    1. Static: Don't change over time (e.g., client_id)\n",
    "    2. Known future: Known at prediction time (e.g., hour, day of week)\n",
    "    3. Observed only: Only known historically (e.g., past consumption)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract time components\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "    df['day_of_month'] = df['datetime'].dt.day\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['week_of_year'] = df['datetime'].dt.isocalendar().week.astype(int)\n",
    "    \n",
    "    # Cyclical encoding (important for periodic features!)\n",
    "    # This helps the model understand that hour 23 is close to hour 0\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    # Create continuous time index per client\n",
    "    df = df.sort_values(['client_id', 'datetime'])\n",
    "    df['time_idx'] = df.groupby('client_id').cumcount()\n",
    "    \n",
    "    # Binary features\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(float)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = add_time_features(df)\n",
    "print(f\"Features: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cyclical encoding\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Hour encoding\n",
    "hours = np.arange(24)\n",
    "hour_sin = np.sin(2 * np.pi * hours / 24)\n",
    "hour_cos = np.cos(2 * np.pi * hours / 24)\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.plot(hours, hour_sin, 'b-', label='sin(hour)', linewidth=2)\n",
    "ax1.plot(hours, hour_cos, 'r-', label='cos(hour)', linewidth=2)\n",
    "ax1.set_xlabel('Hour of Day')\n",
    "ax1.set_ylabel('Encoded Value')\n",
    "ax1.set_title('Cyclical Hour Encoding')\n",
    "ax1.legend()\n",
    "ax1.set_xticks(range(0, 24, 3))\n",
    "\n",
    "# 2D representation\n",
    "ax2 = axes[1]\n",
    "scatter = ax2.scatter(hour_cos, hour_sin, c=hours, cmap='viridis', s=100)\n",
    "for i, h in enumerate(hours):\n",
    "    ax2.annotate(str(h), (hour_cos[i]+0.05, hour_sin[i]+0.05), fontsize=8)\n",
    "ax2.set_xlabel('cos(hour)')\n",
    "ax2.set_ylabel('sin(hour)')\n",
    "ax2.set_title('Hours in 2D Space\\n(Notice: hour 23 is close to hour 0!)')\n",
    "ax2.set_aspect('equal')\n",
    "plt.colorbar(scatter, label='Hour')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWhy cyclical encoding?\")\n",
    "print(\"- Hour 23 should be close to hour 0 (they're adjacent in time)\")\n",
    "print(\"- With simple encoding (0-23), the model sees them as far apart\")\n",
    "print(\"- Cyclical encoding preserves the circular nature of time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Validation/Test Split\n",
    "\n",
    "For time series, we split chronologically to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, train_frac=0.7, val_frac=0.15):\n",
    "    \"\"\"\n",
    "    Split data chronologically into train/val/test sets.\n",
    "    \n",
    "    Important: For time series, we MUST split by time, not randomly!\n",
    "    Random splits would leak future information into training.\n",
    "    \"\"\"\n",
    "    unique_times = sorted(df['time_idx'].unique())\n",
    "    n_times = len(unique_times)\n",
    "    \n",
    "    train_end = int(n_times * train_frac)\n",
    "    val_end = int(n_times * (train_frac + val_frac))\n",
    "    \n",
    "    train_times = set(unique_times[:train_end])\n",
    "    val_times = set(unique_times[train_end:val_end])\n",
    "    test_times = set(unique_times[val_end:])\n",
    "    \n",
    "    train_df = df[df['time_idx'].isin(train_times)].copy()\n",
    "    val_df = df[df['time_idx'].isin(val_times)].copy()\n",
    "    test_df = df[df['time_idx'].isin(test_times)].copy()\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = split_data(df)\n",
    "\n",
    "print(f\"Train: {len(train_df):,} samples ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Val:   {len(val_df):,} samples ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test:  {len(test_df):,} samples ({len(test_df)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the split\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "\n",
    "client = clients[0]\n",
    "train_client = train_df[train_df['client_id'] == client]\n",
    "val_client = val_df[val_df['client_id'] == client]\n",
    "test_client = test_df[test_df['client_id'] == client]\n",
    "\n",
    "ax.plot(train_client['datetime'], train_client['consumption'], \n",
    "        'b-', alpha=0.7, label='Train')\n",
    "ax.plot(val_client['datetime'], val_client['consumption'], \n",
    "        'orange', alpha=0.7, label='Validation')\n",
    "ax.plot(test_client['datetime'], test_client['consumption'], \n",
    "        'g-', alpha=0.7, label='Test')\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Consumption')\n",
    "ax.set_title(f'Train/Val/Test Split for {client}')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Configuration\n",
    "\n",
    "Now let's configure the TFT model. The key decisions are:\n",
    "1. **Input types**: Which features are static, known future, or observed only\n",
    "2. **Sequence lengths**: How much history and how far to forecast\n",
    "3. **Architecture**: Hidden size, attention heads, LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TFT configuration\n",
    "config = TFTConfig(\n",
    "    # === Input Features ===\n",
    "    # Static: Don't change over time (one value per time series)\n",
    "    static_variables=['client_id_num'],\n",
    "    \n",
    "    # Known future: We know these values for future time steps\n",
    "    # (time-based features are always known in advance)\n",
    "    known_future=[\n",
    "        'hour_sin', 'hour_cos',\n",
    "        'day_of_week_sin', 'day_of_week_cos',\n",
    "        'month_sin', 'month_cos',\n",
    "        'is_weekend',\n",
    "    ],\n",
    "    \n",
    "    # Observed only: Only available in historical data\n",
    "    # (the target itself is observed only)\n",
    "    observed_only=[],\n",
    "    \n",
    "    # Target variable to forecast\n",
    "    target='consumption',\n",
    "    \n",
    "    # === Sequence Configuration ===\n",
    "    encoder_length=ENCODER_LENGTH,  # How much history to use\n",
    "    decoder_length=DECODER_LENGTH,  # How far to forecast\n",
    "    \n",
    "    # === Architecture ===\n",
    "    hidden_size=64,      # Size of hidden layers\n",
    "    num_heads=4,         # Number of attention heads\n",
    "    num_lstm_layers=1,   # Number of LSTM layers\n",
    "    dropout=0.1,         # Dropout rate\n",
    "    \n",
    "    # === Quantiles for Probabilistic Forecasting ===\n",
    "    # Output prediction intervals, not just point estimates\n",
    "    quantiles=[0.1, 0.5, 0.9],  # 10th, 50th (median), 90th percentile\n",
    "    \n",
    "    # === Training ===\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-3,\n",
    "    max_epochs=3,  # Reduced for tutorial\n",
    "    gradient_clip_val=1.0,\n",
    ")\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Static variables:    {config.static_variables}\")\n",
    "print(f\"  Known future:        {config.known_future}\")\n",
    "print(f\"  Target:              {config.target}\")\n",
    "print(f\"  Encoder length:      {config.encoder_length}\")\n",
    "print(f\"  Decoder length:      {config.decoder_length}\")\n",
    "print(f\"  Hidden size:         {config.hidden_size}\")\n",
    "print(f\"  Attention heads:     {config.num_heads}\")\n",
    "print(f\"  Quantiles:           {config.quantiles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Input Types\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                         Time Axis                               │\n",
    "│   ◄──────── Encoder (History) ────────►│◄─ Decoder (Future) ─►│\n",
    "├─────────────────────────────────────────┼───────────────────────┤\n",
    "│                                         │                       │\n",
    "│ Static Variables (e.g., client_id)      │                       │\n",
    "│ ════════════════════════════════════════════════════════════    │\n",
    "│ Same value across all time steps        │                       │\n",
    "│                                         │                       │\n",
    "│ Known Future (e.g., hour, day_of_week)  │                       │\n",
    "│ ────────────────────────────────────────────────────────────    │\n",
    "│ Available for both history and future   │                       │\n",
    "│                                         │                       │\n",
    "│ Observed Only (e.g., past consumption)  │                       │\n",
    "│ ────────────────────────────────────────│ ? ? ? ? ? ?           │\n",
    "│ Only available in history               │ Unknown in future     │\n",
    "│                                         │                       │\n",
    "└─────────────────────────────────────────┴───────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create DataLoaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "print(\"Creating data loaders...\")\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    train_data=train_df,\n",
    "    val_data=val_df,\n",
    "    test_data=test_df,\n",
    "    config=config,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches:   {len(val_loader)}\")\n",
    "print(f\"Test batches:  {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Batch contents:\")\n",
    "for key, value in batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TFT model\n",
    "print(\"Creating TFT model...\")\n",
    "model = TemporalFusionTransformer(config)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters:     {num_params:,}\")\n",
    "print(f\"  Trainable parameters: {num_trainable:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training\n",
    "\n",
    "Now let's train the model. We use:\n",
    "- **Quantile Loss**: For probabilistic forecasting\n",
    "- **Early Stopping**: To prevent overfitting\n",
    "- **Model Checkpointing**: To save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = TFTTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        min_delta=1e-4,\n",
    "        verbose=True,\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath='tft_electricity_best.pth',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=True,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Starting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=config.max_epochs,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curves\n",
    "ax1 = axes[0]\n",
    "epochs = range(1, len(trainer.history['train_loss']) + 1)\n",
    "ax1.plot(epochs, trainer.history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "ax1.plot(epochs, trainer.history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate (if available)\n",
    "ax2 = axes[1]\n",
    "if 'learning_rate' in trainer.history:\n",
    "    ax2.plot(epochs, trainer.history['learning_rate'], 'g-', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Learning Rate')\n",
    "    ax2.set_title('Learning Rate Schedule')\n",
    "else:\n",
    "    # Show loss difference\n",
    "    loss_diff = np.array(trainer.history['train_loss']) - np.array(trainer.history['val_loss'])\n",
    "    colors = ['green' if d < 0 else 'red' for d in loss_diff]\n",
    "    ax2.bar(epochs, loss_diff, color=colors, alpha=0.7)\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Train - Val Loss')\n",
    "    ax2.set_title('Overfitting Indicator\\n(Positive = Overfitting)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Train Loss: {trainer.history['train_loss'][-1]:.6f}\")\n",
    "print(f\"  Val Loss:   {trainer.history['val_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation\n",
    "\n",
    "Let's evaluate the model on the test set and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_metrics = trainer.validate(test_loader)\n",
    "\n",
    "print(\"Test Set Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"  {metric:25s}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key metrics\n",
    "metrics_to_plot = {\n",
    "    'MAE': test_metrics.get('mae', 0),\n",
    "    'RMSE': test_metrics.get('rmse', 0),\n",
    "    'R²': test_metrics.get('r2', 0),\n",
    "    'SMAPE': test_metrics.get('smape', 0),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Error metrics\n",
    "ax1 = axes[0]\n",
    "error_metrics = ['MAE', 'RMSE', 'SMAPE']\n",
    "error_values = [metrics_to_plot[m] for m in error_metrics]\n",
    "bars = ax1.bar(error_metrics, error_values, color=['steelblue', 'coral', 'seagreen'])\n",
    "ax1.set_ylabel('Value')\n",
    "ax1.set_title('Error Metrics (Lower is Better)')\n",
    "for bar, val in zip(bars, error_values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Quantile coverage\n",
    "ax2 = axes[1]\n",
    "q_names = ['10% Quantile', '50% Quantile', '90% Quantile']\n",
    "q_coverage = [\n",
    "    test_metrics.get('q10_coverage', 0),\n",
    "    test_metrics.get('q50_coverage', 0),\n",
    "    test_metrics.get('q90_coverage', 0),\n",
    "]\n",
    "expected = [0.1, 0.5, 0.9]\n",
    "\n",
    "x = np.arange(len(q_names))\n",
    "width = 0.35\n",
    "bars1 = ax2.bar(x - width/2, q_coverage, width, label='Actual', color='steelblue')\n",
    "bars2 = ax2.bar(x + width/2, expected, width, label='Expected', color='lightgray')\n",
    "ax2.set_ylabel('Coverage')\n",
    "ax2.set_title('Quantile Coverage\\n(Actual vs Expected)')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(q_names)\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"  R² = {metrics_to_plot['R²']:.3f} means the model explains {metrics_to_plot['R²']*100:.1f}% of variance\")\n",
    "print(f\"  SMAPE = {metrics_to_plot['SMAPE']:.1f}% average percentage error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Predictions Visualization\n",
    "\n",
    "Let's visualize the model's predictions with uncertainty intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "results = trainer.predict(\n",
    "    test_loader,\n",
    "    return_attention=True,\n",
    "    return_variable_selection=True,\n",
    ")\n",
    "\n",
    "predictions = results['predictions']  # Shape: (samples, horizon, quantiles)\n",
    "targets = results['targets']          # Shape: (samples, horizon)\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"Targets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast(predictions, targets, sample_idx, config):\n",
    "    \"\"\"\n",
    "    Plot a single forecast with prediction intervals.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    horizon = predictions.shape[1]\n",
    "    time_steps = np.arange(horizon)\n",
    "    \n",
    "    pred = predictions[sample_idx].numpy()\n",
    "    target = targets[sample_idx].numpy()\n",
    "    \n",
    "    # Plot prediction intervals\n",
    "    q10 = pred[:, 0]  # 10th percentile\n",
    "    q50 = pred[:, 1]  # 50th percentile (median)\n",
    "    q90 = pred[:, 2]  # 90th percentile\n",
    "    \n",
    "    # Confidence interval\n",
    "    ax.fill_between(time_steps, q10, q90, alpha=0.3, color='blue', \n",
    "                    label='80% Prediction Interval')\n",
    "    \n",
    "    # Median prediction\n",
    "    ax.plot(time_steps, q50, 'b-', linewidth=2, label='Median Prediction (q50)')\n",
    "    \n",
    "    # Actual values\n",
    "    ax.plot(time_steps, target, 'ro-', linewidth=2, markersize=8, \n",
    "            label='Actual Values')\n",
    "    \n",
    "    ax.set_xlabel('Forecast Horizon (steps)')\n",
    "    ax.set_ylabel('Consumption')\n",
    "    ax.set_title(f'TFT Forecast - Sample {sample_idx}')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add step labels\n",
    "    ax.set_xticks(time_steps)\n",
    "    ax.set_xticklabels([f'+{(i+1)*4}h' for i in time_steps])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot a few samples\n",
    "for idx in [0, 100, 500]:\n",
    "    if idx < len(predictions):\n",
    "        plot_forecast(predictions, targets, idx, config)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multiple forecasts together\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "sample_indices = [0, 50, 150, 300]\n",
    "\n",
    "for ax, idx in zip(axes, sample_indices):\n",
    "    if idx >= len(predictions):\n",
    "        continue\n",
    "        \n",
    "    horizon = predictions.shape[1]\n",
    "    time_steps = np.arange(horizon)\n",
    "    \n",
    "    pred = predictions[idx].numpy()\n",
    "    target = targets[idx].numpy()\n",
    "    \n",
    "    q10, q50, q90 = pred[:, 0], pred[:, 1], pred[:, 2]\n",
    "    \n",
    "    ax.fill_between(time_steps, q10, q90, alpha=0.3, color='blue')\n",
    "    ax.plot(time_steps, q50, 'b-', linewidth=2, label='Prediction')\n",
    "    ax.plot(time_steps, target, 'ro-', linewidth=2, markersize=6, label='Actual')\n",
    "    \n",
    "    ax.set_xlabel('Horizon')\n",
    "    ax.set_ylabel('Consumption')\n",
    "    ax.set_title(f'Sample {idx}')\n",
    "    ax.legend(loc='best', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('TFT Forecasts Across Different Samples', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Predicted vs Actual\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Use median predictions\n",
    "pred_median = predictions[:, :, 1].flatten().numpy()\n",
    "actual = targets.flatten().numpy()\n",
    "\n",
    "# Sample for visualization (too many points otherwise)\n",
    "n_sample = min(5000, len(pred_median))\n",
    "indices = np.random.choice(len(pred_median), n_sample, replace=False)\n",
    "\n",
    "ax.scatter(actual[indices], pred_median[indices], alpha=0.3, s=10)\n",
    "\n",
    "# Perfect prediction line\n",
    "min_val = min(actual.min(), pred_median.min())\n",
    "max_val = max(actual.max(), pred_median.max())\n",
    "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "\n",
    "ax.set_xlabel('Actual Consumption')\n",
    "ax.set_ylabel('Predicted Consumption')\n",
    "ax.set_title('Predicted vs Actual Values')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = np.corrcoef(actual, pred_median)[0, 1]\n",
    "print(f\"Correlation between predicted and actual: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by forecast horizon\n",
    "errors_by_horizon = []\n",
    "for h in range(predictions.shape[1]):\n",
    "    pred_h = predictions[:, h, 1].numpy()  # Median prediction\n",
    "    actual_h = targets[:, h].numpy()\n",
    "    mae_h = np.abs(pred_h - actual_h).mean()\n",
    "    errors_by_horizon.append(mae_h)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "horizons = range(1, len(errors_by_horizon) + 1)\n",
    "ax.bar(horizons, errors_by_horizon, color='steelblue', alpha=0.8)\n",
    "ax.set_xlabel('Forecast Horizon (steps)')\n",
    "ax.set_ylabel('Mean Absolute Error')\n",
    "ax.set_title('Prediction Error by Forecast Horizon\\n(Error typically increases with horizon)')\n",
    "ax.set_xticks(horizons)\n",
    "ax.set_xticklabels([f'+{h*4}h' for h in horizons])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: Error tends to increase with forecast horizon\")\n",
    "print(\"This is expected - predicting further ahead is harder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution\n",
    "errors = (predictions[:, :, 1] - targets).flatten().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Histogram\n",
    "ax1 = axes[0]\n",
    "ax1.hist(errors, bins=50, density=True, alpha=0.7, color='steelblue')\n",
    "ax1.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "ax1.axvline(x=errors.mean(), color='green', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {errors.mean():.3f}')\n",
    "ax1.set_xlabel('Prediction Error')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Error Distribution')\n",
    "ax1.legend()\n",
    "\n",
    "# Q-Q plot\n",
    "from scipy import stats\n",
    "ax2 = axes[1]\n",
    "stats.probplot(errors, dist=\"norm\", plot=ax2)\n",
    "ax2.set_title('Q-Q Plot (Normality Check)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Error Statistics:\")\n",
    "print(f\"  Mean:     {errors.mean():.4f}\")\n",
    "print(f\"  Std:      {errors.std():.4f}\")\n",
    "print(f\"  Skewness: {stats.skew(errors):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Interpretability\n",
    "\n",
    "One of TFT's key strengths is interpretability. Let's examine:\n",
    "1. **Variable Selection Weights**: Which features matter most?\n",
    "2. **Attention Weights**: Which time steps are most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what interpretability outputs are available\n",
    "print(\"Available interpretation outputs:\")\n",
    "for key in results.keys():\n",
    "    if 'attention' in key.lower() or 'variable' in key.lower() or 'weight' in key.lower():\n",
    "        value = results[key]\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"  {key}: shape={value.shape}\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(f\"  {key}: dict with keys {list(value.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot attention weights if available\n",
    "if 'attention_weights' in results:\n",
    "    attention = results['attention_weights']\n",
    "    \n",
    "    # Average attention across samples\n",
    "    if isinstance(attention, torch.Tensor):\n",
    "        avg_attention = attention.mean(dim=0).numpy()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        im = ax.imshow(avg_attention, aspect='auto', cmap='Blues')\n",
    "        ax.set_xlabel('Encoder Time Steps (History)')\n",
    "        ax.set_ylabel('Decoder Time Steps (Future)')\n",
    "        ax.set_title('Average Attention Weights\\n(Which historical steps matter for each future prediction?)')\n",
    "        plt.colorbar(im, label='Attention Weight')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nInterpretation:\")\n",
    "        print(\"  Brighter colors = higher attention = more important time steps\")\n",
    "        print(\"  The model learns which historical patterns are most predictive\")\n",
    "else:\n",
    "    print(\"Attention weights not available in results\")\n",
    "    print(\"This might be due to the model configuration or batch size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **TFT Architecture**: Combines LSTMs, attention, and variable selection for interpretable forecasting\n",
    "2. **Feature Types**: Static, known future, and observed-only features require different handling\n",
    "3. **Probabilistic Forecasting**: Quantile outputs provide uncertainty estimates\n",
    "4. **Cyclical Encoding**: Important for time-based features\n",
    "\n",
    "### Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"                    TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset: ElectricityLoadDiagrams20112014\")\n",
    "print(f\"Clients: {NUM_CLIENTS}\")\n",
    "print(f\"Resampling: {RESAMPLE_FREQ}\")\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Encoder length: {ENCODER_LENGTH} steps\")\n",
    "print(f\"  Decoder length: {DECODER_LENGTH} steps\")\n",
    "print(f\"  Hidden size: {config.hidden_size}\")\n",
    "print(f\"  Parameters: {num_params:,}\")\n",
    "print(f\"\\nTraining Results:\")\n",
    "print(f\"  Final train loss: {trainer.history['train_loss'][-1]:.6f}\")\n",
    "print(f\"  Final val loss:   {trainer.history['val_loss'][-1]:.6f}\")\n",
    "print(f\"\\nTest Metrics:\")\n",
    "print(f\"  MAE:   {test_metrics.get('mae', 0):.4f}\")\n",
    "print(f\"  RMSE:  {test_metrics.get('rmse', 0):.4f}\")\n",
    "print(f\"  R²:    {test_metrics.get('r2', 0):.4f}\")\n",
    "print(f\"  SMAPE: {test_metrics.get('smape', 0):.2f}%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Results\n",
    "\n",
    "To get better performance, try:\n",
    "\n",
    "1. **More data**: Increase `NUM_CLIENTS` and use hourly resampling\n",
    "2. **Larger model**: Increase `hidden_size` to 128 or 256\n",
    "3. **More epochs**: Train for 20-50 epochs with early stopping\n",
    "4. **Additional features**: Add weather data, holidays, etc.\n",
    "5. **GPU acceleration**: Use CUDA for faster training\n",
    "\n",
    "### References\n",
    "\n",
    "- [Original TFT Paper](https://arxiv.org/abs/1912.09363): \"Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting\"\n",
    "- [UCI Dataset](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTutorial complete! Happy forecasting!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
