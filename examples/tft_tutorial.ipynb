{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Fusion Transformer (TFT) - Complete Tutorial\n",
    "\n",
    "This notebook provides a comprehensive guide to using the Temporal Fusion Transformer for time series forecasting. We'll cover:\n",
    "\n",
    "1. **Understanding TFT Architecture** - What makes TFT special\n",
    "2. **Data Preparation** - Generating and preparing time series data\n",
    "3. **Model Configuration** - Setting up the model parameters\n",
    "4. **Training** - Training the model with callbacks\n",
    "5. **Prediction** - Making forecasts\n",
    "6. **Interpretability** - Understanding model decisions through attention and variable importance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TFT Architecture Overview\n",
    "\n",
    "The Temporal Fusion Transformer (TFT) is a state-of-the-art deep learning architecture for **interpretable multi-horizon time series forecasting**.\n",
    "\n",
    "### Key Features:\n",
    "- **Multi-horizon forecasting**: Predicts multiple future time steps simultaneously\n",
    "- **Probabilistic predictions**: Outputs quantiles for uncertainty estimation\n",
    "- **Interpretability**: Built-in attention mechanisms and variable selection\n",
    "- **Heterogeneous inputs**: Handles static, known future, and observed-only features\n",
    "\n",
    "### Architecture Flow:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         TEMPORAL FUSION TRANSFORMER                          │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "              ┌──────────────┐   ┌──────────────┐   ┌──────────────┐\n",
    "              │   STATIC     │   │ KNOWN FUTURE │   │  OBSERVED    │\n",
    "              │  FEATURES    │   │   FEATURES   │   │   FEATURES   │\n",
    "              │  (e.g., ID)  │   │ (e.g., time) │   │ (e.g., temp) │\n",
    "              └──────┬───────┘   └──────┬───────┘   └──────┬───────┘\n",
    "                     │                  │                  │\n",
    "                     ▼                  │                  │\n",
    "        ┌────────────────────────┐      │                  │\n",
    "        │   STATIC COVARIATE     │      │                  │\n",
    "        │       ENCODER          │      │                  │\n",
    "        │  (Creates 4 context    │      │                  │\n",
    "        │   vectors for gating)  │      │                  │\n",
    "        └───────────┬────────────┘      │                  │\n",
    "                    │                   │                  │\n",
    "         Context    │    ┌──────────────┴──────────────────┤\n",
    "         Vectors    │    │                                 │\n",
    "                    ▼    ▼                                 ▼\n",
    "        ┌─────────────────────────────────────────────────────────────┐\n",
    "        │              VARIABLE SELECTION NETWORKS                     │\n",
    "        │   ┌─────────────────────┐   ┌─────────────────────┐         │\n",
    "        │   │  Encoder Variables  │   │  Decoder Variables  │         │\n",
    "        │   │  (Historical data)  │   │  (Future known)     │         │\n",
    "        │   │                     │   │                     │         │\n",
    "        │   │  Softmax weights    │   │  Softmax weights    │         │\n",
    "        │   │  select important   │   │  select important   │         │\n",
    "        │   │  features           │   │  features           │         │\n",
    "        │   └──────────┬──────────┘   └──────────┬──────────┘         │\n",
    "        └──────────────┼──────────────────────────┼───────────────────┘\n",
    "                       │                          │\n",
    "                       ▼                          ▼\n",
    "        ┌─────────────────────────────────────────────────────────────┐\n",
    "        │                    LSTM ENCODER-DECODER                      │\n",
    "        │   ┌─────────────────────┐   ┌─────────────────────┐         │\n",
    "        │   │    LSTM Encoder     │──▶│    LSTM Decoder     │         │\n",
    "        │   │  (Processes past)   │   │  (Processes future) │         │\n",
    "        │   └─────────────────────┘   └─────────────────────┘         │\n",
    "        └──────────────────────────────┬──────────────────────────────┘\n",
    "                                       │\n",
    "                                       ▼\n",
    "        ┌─────────────────────────────────────────────────────────────┐\n",
    "        │              STATIC ENRICHMENT LAYER                         │\n",
    "        │         (Enhances temporal features with                     │\n",
    "        │          static context via GRN)                             │\n",
    "        └──────────────────────────────┬──────────────────────────────┘\n",
    "                                       │\n",
    "                                       ▼\n",
    "        ┌─────────────────────────────────────────────────────────────┐\n",
    "        │           INTERPRETABLE MULTI-HEAD ATTENTION                 │\n",
    "        │                                                              │\n",
    "        │   • Uses ADDITIVE attention (not dot-product)               │\n",
    "        │   • Allows inspection of temporal relationships             │\n",
    "        │   • Shows which past time steps matter most                 │\n",
    "        │                                                              │\n",
    "        └──────────────────────────────┬──────────────────────────────┘\n",
    "                                       │\n",
    "                                       ▼\n",
    "        ┌─────────────────────────────────────────────────────────────┐\n",
    "        │              POSITION-WISE FEED-FORWARD                      │\n",
    "        │               (GRN for each position)                        │\n",
    "        └──────────────────────────────┬──────────────────────────────┘\n",
    "                                       │\n",
    "                                       ▼\n",
    "        ┌─────────────────────────────────────────────────────────────┐\n",
    "        │                 QUANTILE OUTPUT HEADS                        │\n",
    "        │                                                              │\n",
    "        │   ┌─────────┐  ┌─────────┐  ┌─────────┐                     │\n",
    "        │   │  Q=0.1  │  │  Q=0.5  │  │  Q=0.9  │   ...               │\n",
    "        │   │ (Lower) │  │(Median) │  │ (Upper) │                     │\n",
    "        │   └────┬────┘  └────┬────┘  └────┬────┘                     │\n",
    "        └────────┼────────────┼────────────┼──────────────────────────┘\n",
    "                 │            │            │\n",
    "                 ▼            ▼            ▼\n",
    "        ┌─────────────────────────────────────────────────────────────┐\n",
    "        │                    PREDICTIONS                               │\n",
    "        │        (Multi-horizon probabilistic forecasts)               │\n",
    "        └─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Building Blocks:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| **GRN (Gated Residual Network)** | Non-linear processing with skip connections |\n",
    "| **GLU (Gated Linear Unit)** | Controls information flow through gating |\n",
    "| **Variable Selection Network** | Learns which features are important |\n",
    "| **Multi-Head Attention** | Captures temporal dependencies |\n",
    "| **Quantile Outputs** | Provides prediction intervals |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports\n",
    "\n",
    "Let's start by importing the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TFT imports\n",
    "from tft.models import TemporalFusionTransformer\n",
    "from tft.data import create_dataloaders\n",
    "from tft.training import TFTTrainer, EarlyStopping, ModelCheckpoint\n",
    "from tft.utils import TFTConfig, get_device, print_device_info\n",
    "from tft.utils.visualization import (\n",
    "    plot_predictions, \n",
    "    plot_training_history,\n",
    "    plot_attention_weights,\n",
    "    plot_variable_importance,\n",
    "    plot_forecast_fan,\n",
    ")\n",
    "from tft.interpret import (\n",
    "    extract_attention_weights,\n",
    "    average_attention_over_heads,\n",
    "    plot_attention_heatmap,\n",
    "    plot_attention_by_head,\n",
    "    analyze_attention_focus,\n",
    "    extract_variable_selection_weights,\n",
    "    rank_variables_by_importance,\n",
    "    plot_temporal_variable_importance,\n",
    ")\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Check available device\n",
    "print(\"=\" * 60)\n",
    "print(\"Device Information\")\n",
    "print(\"=\" * 60)\n",
    "print_device_info()\n",
    "device = get_device('auto')\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Generation\n",
    "\n",
    "We'll create synthetic time series data that demonstrates TFT's ability to handle:\n",
    "- **Static features**: Time-invariant characteristics (e.g., series ID)\n",
    "- **Known future features**: Features known at forecast time (e.g., time of day)\n",
    "- **Observed features**: Historical-only features (e.g., temperature)\n",
    "\n",
    "### Data Structure:\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    TIME SERIES DATA                          │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│  series_id │ time_idx │ target │ hour_sin │ temperature │   │\n",
    "│  (static)  │          │        │ (known)  │ (observed)  │   │\n",
    "├────────────┼──────────┼────────┼──────────┼─────────────┤   │\n",
    "│     0      │    0     │  5.2   │   0.26   │    21.3     │   │\n",
    "│     0      │    1     │  5.8   │   0.50   │    22.1     │   │\n",
    "│     0      │    2     │  6.1   │   0.71   │    23.0     │   │\n",
    "│    ...     │   ...    │  ...   │   ...    │    ...      │   │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(\n",
    "    num_samples: int = 1000,\n",
    "    num_series: int = 5,\n",
    "    noise_level: float = 0.1,\n",
    "    seed: int = 42,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic time series data with trend, seasonality, and noise.\n",
    "    \n",
    "    The data simulates multiple related time series (e.g., different stores/sensors)\n",
    "    with common patterns but individual characteristics.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    data_list = []\n",
    "\n",
    "    for series_id in range(num_series):\n",
    "        # Time index\n",
    "        time_idx = np.arange(num_samples)\n",
    "\n",
    "        # Components of the target signal\n",
    "        trend = 0.02 * time_idx  # Linear trend\n",
    "        seasonality = 10 * np.sin(2 * np.pi * time_idx / 50)  # Seasonal pattern (period=50)\n",
    "        noise = noise_level * np.random.randn(num_samples)  # Random noise\n",
    "\n",
    "        # Target value: combination of components + series-specific offset\n",
    "        target = trend + seasonality + noise + series_id * 2\n",
    "\n",
    "        # Known future features (time-based, known at forecast time)\n",
    "        hour = (time_idx % 24).astype(float)\n",
    "        day_of_week = ((time_idx // 24) % 7).astype(float)\n",
    "        \n",
    "        # Cyclical encoding for hour\n",
    "        hour_sin = np.sin(2 * np.pi * hour / 24)\n",
    "        hour_cos = np.cos(2 * np.pi * hour / 24)\n",
    "\n",
    "        # Observed feature (correlated with target, only known historically)\n",
    "        temperature = 20 + 5 * np.sin(2 * np.pi * time_idx / 50) + np.random.randn(num_samples) * 0.5\n",
    "\n",
    "        series_df = pd.DataFrame({\n",
    "            'series_id': series_id,\n",
    "            'time_idx': time_idx,\n",
    "            'target': target,\n",
    "            'hour': hour,\n",
    "            'day_of_week': day_of_week,\n",
    "            'hour_sin': hour_sin,\n",
    "            'hour_cos': hour_cos,\n",
    "            'temperature': temperature,\n",
    "        })\n",
    "        data_list.append(series_df)\n",
    "\n",
    "    return pd.concat(data_list, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "# Generate data\n",
    "df = generate_synthetic_data(num_samples=1000, num_series=5, noise_level=0.5, seed=42)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of series: {df['series_id'].nunique()}\")\n",
    "print(f\"Time steps per series: {df.groupby('series_id').size().iloc[0]}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data\n",
    "\n",
    "Let's visualize the synthetic time series to understand its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Target for all series\n",
    "ax1 = axes[0, 0]\n",
    "for series_id in df['series_id'].unique():\n",
    "    series_data = df[df['series_id'] == series_id]\n",
    "    ax1.plot(series_data['time_idx'], series_data['target'], label=f'Series {series_id}', alpha=0.7)\n",
    "ax1.set_xlabel('Time Index')\n",
    "ax1.set_ylabel('Target Value')\n",
    "ax1.set_title('Target Time Series (All Series)')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Single series with components\n",
    "ax2 = axes[0, 1]\n",
    "series_0 = df[df['series_id'] == 0].head(200)\n",
    "ax2.plot(series_0['time_idx'], series_0['target'], 'b-', label='Target', linewidth=2)\n",
    "ax2.plot(series_0['time_idx'], series_0['temperature'], 'r--', label='Temperature', alpha=0.7)\n",
    "ax2.set_xlabel('Time Index')\n",
    "ax2.set_ylabel('Value')\n",
    "ax2.set_title('Series 0: Target vs Temperature (Correlated)')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Known future features (cyclical encoding)\n",
    "ax3 = axes[1, 0]\n",
    "sample = df[df['series_id'] == 0].head(100)\n",
    "ax3.plot(sample['time_idx'], sample['hour_sin'], 'g-', label='Hour (sin)', linewidth=2)\n",
    "ax3.plot(sample['time_idx'], sample['hour_cos'], 'orange', label='Hour (cos)', linewidth=2)\n",
    "ax3.set_xlabel('Time Index')\n",
    "ax3.set_ylabel('Encoded Value')\n",
    "ax3.set_title('Known Future Features (Cyclical Hour Encoding)')\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Distribution of target\n",
    "ax4 = axes[1, 1]\n",
    "for series_id in df['series_id'].unique():\n",
    "    series_data = df[df['series_id'] == series_id]\n",
    "    ax4.hist(series_data['target'], bins=30, alpha=0.5, label=f'Series {series_id}')\n",
    "ax4.set_xlabel('Target Value')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.set_title('Distribution of Target Values')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Splitting\n",
    "\n",
    "We split the data temporally to ensure no data leakage:\n",
    "\n",
    "```\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│              TRAIN (70%)            │   VAL (15%)   │  TEST (15%)  │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "                                      ↑               ↑\n",
    "                               train_end_idx    val_end_idx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Split data temporally: 70% train, 15% val, 15% test.\"\"\"\n",
    "    unique_times = sorted(df['time_idx'].unique())\n",
    "    n_times = len(unique_times)\n",
    "\n",
    "    train_end = int(n_times * 0.7)\n",
    "    val_end = int(n_times * 0.85)\n",
    "\n",
    "    train_times = unique_times[:train_end]\n",
    "    val_times = unique_times[train_end:val_end]\n",
    "    test_times = unique_times[val_end:]\n",
    "\n",
    "    train_df = df[df['time_idx'].isin(train_times)].copy()\n",
    "    val_df = df[df['time_idx'].isin(val_times)].copy()\n",
    "    test_df = df[df['time_idx'].isin(test_times)].copy()\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "train_df, val_df, test_df = prepare_data(df)\n",
    "\n",
    "print(\"Data Split Summary:\")\n",
    "print(f\"  Train: {len(train_df):,} samples ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_df):,} samples ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_df):,} samples ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nTime ranges:\")\n",
    "print(f\"  Train: {train_df['time_idx'].min()} - {train_df['time_idx'].max()}\")\n",
    "print(f\"  Val:   {val_df['time_idx'].min()} - {val_df['time_idx'].max()}\")\n",
    "print(f\"  Test:  {test_df['time_idx'].min()} - {test_df['time_idx'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Configuration\n",
    "\n",
    "TFT requires careful configuration of:\n",
    "1. **Input features** - Which features are static, known future, or observed-only\n",
    "2. **Sequence lengths** - How much history to use (encoder) and how far to forecast (decoder)\n",
    "3. **Architecture** - Hidden dimensions, attention heads, LSTM layers\n",
    "4. **Quantiles** - Which prediction intervals to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TFTConfig(\n",
    "    # === Input Features ===\n",
    "    # Static: Time-invariant features (constant for each series)\n",
    "    static_variables=['series_id'],\n",
    "    \n",
    "    # Known Future: Features known at forecast time (e.g., calendar features)\n",
    "    known_future=['hour_sin', 'hour_cos', 'day_of_week'],\n",
    "    \n",
    "    # Observed Only: Features only available historically (not known in future)\n",
    "    observed_only=['temperature'],\n",
    "    \n",
    "    # Target variable to forecast\n",
    "    target='target',\n",
    "\n",
    "    # === Sequence Configuration ===\n",
    "    encoder_length=50,   # Look back 50 time steps\n",
    "    decoder_length=10,   # Forecast 10 steps ahead\n",
    "\n",
    "    # === Architecture ===\n",
    "    hidden_size=64,      # Hidden dimension (larger = more capacity)\n",
    "    num_heads=4,         # Number of attention heads\n",
    "    num_lstm_layers=1,   # LSTM depth\n",
    "    dropout=0.1,         # Regularization\n",
    "\n",
    "    # === Quantiles for Probabilistic Forecasting ===\n",
    "    quantiles=[0.1, 0.5, 0.9],  # 10th, 50th (median), 90th percentile\n",
    "\n",
    "    # === Training ===\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-3,\n",
    "    max_epochs=20,  # Increased for better convergence\n",
    "    gradient_clip_val=1.0,\n",
    ")\n",
    "\n",
    "print(\"TFT Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nInput Features:\")\n",
    "print(f\"  Static variables:    {config.static_variables}\")\n",
    "print(f\"  Known future:        {config.known_future}\")\n",
    "print(f\"  Observed only:       {config.observed_only}\")\n",
    "print(f\"  Target:              {config.target}\")\n",
    "print(f\"\\nSequence Configuration:\")\n",
    "print(f\"  Encoder length:      {config.encoder_length} steps\")\n",
    "print(f\"  Decoder length:      {config.decoder_length} steps\")\n",
    "print(f\"  Total window:        {config.encoder_length + config.decoder_length} steps\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  Hidden size:         {config.hidden_size}\")\n",
    "print(f\"  Attention heads:     {config.num_heads}\")\n",
    "print(f\"  LSTM layers:         {config.num_lstm_layers}\")\n",
    "print(f\"  Dropout:             {config.dropout}\")\n",
    "print(f\"\\nQuantiles:             {config.quantiles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Encoder/Decoder Split\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         INPUT SEQUENCE WINDOW                              │\n",
    "├────────────────────────────────────┬───────────────────────────────────────┤\n",
    "│         ENCODER (Historical)       │        DECODER (Forecast)             │\n",
    "│         encoder_length = 50        │        decoder_length = 10            │\n",
    "├────────────────────────────────────┼───────────────────────────────────────┤\n",
    "│                                    │                                       │\n",
    "│  ← Static features available →     │  ← Static features available →        │\n",
    "│  ← Known future features →         │  ← Known future features →            │\n",
    "│  ← Observed features →             │  ✗ Observed features NOT available    │\n",
    "│  ← Target (historical) →           │  ← Target (to predict) →              │\n",
    "│                                    │                                       │\n",
    "│     t-49  t-48  ...  t-1  t=0      │     t+1  t+2  ...  t+9  t+10         │\n",
    "└────────────────────────────────────┴───────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Data Loaders\n",
    "\n",
    "Data loaders handle windowing the time series into sequences suitable for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    train_data=train_df,\n",
    "    val_data=val_df,\n",
    "    test_data=test_df,\n",
    "    config=config,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(\"DataLoader Summary:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches:   {len(val_loader)}\")\n",
    "print(f\"  Test batches:  {len(test_loader)}\")\n",
    "\n",
    "# Inspect a batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch contents:\")\n",
    "for key, value in sample_batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create and Inspect the Model\n",
    "\n",
    "Let's create the TFT model and examine its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemporalFusionTransformer(config)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total parameters:     {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training\n",
    "\n",
    "We'll train the model with:\n",
    "- **Early Stopping**: Stop if validation loss doesn't improve\n",
    "- **Model Checkpoint**: Save the best model\n",
    "\n",
    "The loss function is **Quantile Loss** (Pinball Loss), which trains the model to predict multiple quantiles simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = TFTTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        min_delta=1e-4,\n",
    "        verbose=True,\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath='tft_best_model.pth',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=True,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=config.max_epochs,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation loss: {trainer.best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1 = axes[0]\n",
    "epochs = range(1, len(trainer.history['train_loss']) + 1)\n",
    "ax1.plot(epochs, trainer.history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "ax1.plot(epochs, trainer.history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Quantile Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss difference (overfitting indicator)\n",
    "ax2 = axes[1]\n",
    "loss_diff = np.array(trainer.history['val_loss']) - np.array(trainer.history['train_loss'])\n",
    "colors = ['green' if d < 0.1 else 'orange' if d < 0.3 else 'red' for d in loss_diff]\n",
    "ax2.bar(epochs, loss_diff, color=colors, alpha=0.7)\n",
    "ax2.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Val Loss - Train Loss')\n",
    "ax2.set_title('Generalization Gap (Lower is Better)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation and Prediction\n",
    "\n",
    "Let's evaluate the model on the test set and generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_metrics = trainer.validate(test_loader)\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"  {metric}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions with attention and variable selection weights\n",
    "print(\"Generating predictions...\")\n",
    "results = trainer.predict(\n",
    "    test_loader,\n",
    "    return_attention=True,\n",
    "    return_variable_selection=True,\n",
    ")\n",
    "\n",
    "predictions = results['predictions']\n",
    "targets = results['targets']\n",
    "\n",
    "print(f\"\\nPredictions shape: {predictions.shape}\")\n",
    "print(f\"  - Batch dimension: {predictions.shape[0]} samples\")\n",
    "print(f\"  - Time dimension: {predictions.shape[1]} forecast steps\")\n",
    "print(f\"  - Quantile dimension: {predictions.shape[2]} quantiles {config.quantiles}\")\n",
    "print(f\"\\nTargets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions\n",
    "\n",
    "TFT outputs **quantile predictions**, providing uncertainty estimates:\n",
    "- **Q0.1**: 10th percentile (lower bound)\n",
    "- **Q0.5**: 50th percentile (median prediction)\n",
    "- **Q0.9**: 90th percentile (upper bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for multiple samples\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    sample_idx = idx * 10  # Sample every 10th prediction\n",
    "    \n",
    "    pred = predictions[sample_idx].cpu().numpy()\n",
    "    target = targets[sample_idx].cpu().numpy()\n",
    "    \n",
    "    time_steps = np.arange(len(target))\n",
    "    \n",
    "    # Plot actual values\n",
    "    ax.plot(time_steps, target, 'k-', label='Actual', linewidth=2, marker='o', markersize=4)\n",
    "    \n",
    "    # Plot median prediction\n",
    "    median_idx = config.quantiles.index(0.5)\n",
    "    ax.plot(time_steps, pred[:, median_idx], 'b-', label='Predicted (Q0.5)', linewidth=2, marker='s', markersize=4)\n",
    "    \n",
    "    # Plot prediction interval\n",
    "    ax.fill_between(\n",
    "        time_steps,\n",
    "        pred[:, 0],  # Q0.1\n",
    "        pred[:, -1],  # Q0.9\n",
    "        alpha=0.3,\n",
    "        color='blue',\n",
    "        label='80% Prediction Interval'\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel('Forecast Step')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title(f'Sample {sample_idx}')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('TFT Predictions with Uncertainty Intervals', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prediction accuracy metrics\n",
    "pred_np = predictions.cpu().numpy()\n",
    "target_np = targets.cpu().numpy()\n",
    "\n",
    "# Median predictions\n",
    "median_idx = config.quantiles.index(0.5)\n",
    "median_pred = pred_np[:, :, median_idx]\n",
    "\n",
    "# Calculate metrics\n",
    "mae = np.mean(np.abs(median_pred - target_np))\n",
    "rmse = np.sqrt(np.mean((median_pred - target_np) ** 2))\n",
    "mape = np.mean(np.abs((median_pred - target_np) / (target_np + 1e-8))) * 100\n",
    "\n",
    "# Coverage: What % of actuals fall within prediction interval\n",
    "lower = pred_np[:, :, 0]  # Q0.1\n",
    "upper = pred_np[:, :, -1]  # Q0.9\n",
    "coverage = np.mean((target_np >= lower) & (target_np <= upper)) * 100\n",
    "\n",
    "print(\"Prediction Quality Metrics:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"  MAE:  {mae:.4f}\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  MAPE: {mape:.2f}%\")\n",
    "print(f\"\\n  80% Interval Coverage: {coverage:.1f}% (target: 80%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Interpretability\n",
    "\n",
    "One of TFT's key advantages is **built-in interpretability**. Let's examine:\n",
    "1. **Attention Weights**: Which past time steps influence predictions?\n",
    "2. **Variable Importance**: Which features matter most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Attention Weights Analysis\n",
    "\n",
    "Attention weights show how the model weighs different historical time steps when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if attention weights are available\n",
    "if 'attention_weights' in results:\n",
    "    attention = results['attention_weights']\n",
    "    print(f\"Attention weights shape: {attention.shape}\")\n",
    "    print(f\"  - Samples: {attention.shape[0]}\")\n",
    "    print(f\"  - Attention heads: {attention.shape[1]}\")\n",
    "    print(f\"  - Target positions: {attention.shape[2]}\")\n",
    "    print(f\"  - Source positions: {attention.shape[3]}\")\n",
    "else:\n",
    "    print(\"Attention weights not available in results\")\n",
    "    attention = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if attention is not None:\n",
    "    # Average attention across heads for visualization\n",
    "    avg_attention = attention.mean(dim=1)  # Average over heads\n",
    "    \n",
    "    # Plot attention heatmap for a sample\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Single sample attention heatmap\n",
    "    sample_idx = 0\n",
    "    sample_attention = avg_attention[sample_idx].cpu().numpy()\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    im1 = ax1.imshow(sample_attention, cmap='viridis', aspect='auto')\n",
    "    ax1.set_xlabel('Source Position (Historical)')\n",
    "    ax1.set_ylabel('Target Position (Forecast)')\n",
    "    ax1.set_title(f'Attention Heatmap (Sample {sample_idx})')\n",
    "    plt.colorbar(im1, ax=ax1, label='Attention Weight')\n",
    "    \n",
    "    # Average attention pattern across all samples\n",
    "    mean_attention = avg_attention.mean(dim=0).cpu().numpy()  # Average over samples\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    im2 = ax2.imshow(mean_attention, cmap='viridis', aspect='auto')\n",
    "    ax2.set_xlabel('Source Position (Historical)')\n",
    "    ax2.set_ylabel('Target Position (Forecast)')\n",
    "    ax2.set_title('Average Attention Pattern (All Samples)')\n",
    "    plt.colorbar(im2, ax=ax2, label='Attention Weight')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if attention is not None:\n",
    "    # Analyze which historical positions get the most attention\n",
    "    # Average over samples, heads, and target positions\n",
    "    temporal_importance = avg_attention.mean(dim=(0, 1)).cpu().numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    \n",
    "    positions = np.arange(len(temporal_importance))\n",
    "    colors = plt.cm.viridis(temporal_importance / temporal_importance.max())\n",
    "    \n",
    "    ax.bar(positions, temporal_importance, color=colors)\n",
    "    ax.set_xlabel('Historical Time Step')\n",
    "    ax.set_ylabel('Average Attention Weight')\n",
    "    ax.set_title('Temporal Attention Distribution: Which Past Time Steps Matter Most?')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Mark the most important positions\n",
    "    top_k = 5\n",
    "    top_indices = np.argsort(temporal_importance)[-top_k:][::-1]\n",
    "    for idx in top_indices:\n",
    "        ax.annotate(f't-{len(temporal_importance)-idx-1}', \n",
    "                   xy=(idx, temporal_importance[idx]),\n",
    "                   xytext=(idx, temporal_importance[idx] + 0.01),\n",
    "                   ha='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTop {top_k} most attended historical positions:\")\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        print(f\"  {i+1}. Position {idx} (t-{len(temporal_importance)-idx-1}): {temporal_importance[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Variable Importance Analysis\n",
    "\n",
    "TFT's Variable Selection Networks learn which input features are most important for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available variable selection outputs\n",
    "print(\"Available keys in results:\")\n",
    "for key in results.keys():\n",
    "    if isinstance(results[key], torch.Tensor):\n",
    "        print(f\"  {key}: {results[key].shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(results[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable importance visualization\n",
    "# We'll compute importance based on the encoder variable selection\n",
    "\n",
    "if 'encoder_variable_selection' in results:\n",
    "    encoder_weights = results['encoder_variable_selection']\n",
    "    \n",
    "    # Define encoder variable names (observed + known future + target)\n",
    "    encoder_vars = config.observed_only + config.known_future + [config.target]\n",
    "    \n",
    "    # Average importance\n",
    "    avg_importance = encoder_weights.mean(dim=(0, 1)).cpu().numpy()\n",
    "    \n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Variable': encoder_vars[:len(avg_importance)],\n",
    "        'Importance': avg_importance[:len(encoder_vars)]\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = plt.cm.RdYlGn(importance_df['Importance'] / importance_df['Importance'].max())\n",
    "    ax.barh(importance_df['Variable'], importance_df['Importance'], color=colors)\n",
    "    ax.set_xlabel('Selection Weight (Importance)')\n",
    "    ax.set_ylabel('Variable')\n",
    "    ax.set_title('Encoder Variable Importance\\n(Learned by Variable Selection Network)')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nVariable Importance Ranking:\")\n",
    "    for i, (_, row) in enumerate(importance_df.iloc[::-1].iterrows()):\n",
    "        print(f\"  {i+1}. {row['Variable']}: {row['Importance']:.4f}\")\n",
    "else:\n",
    "    print(\"Variable selection weights not available in results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **TFT Architecture**: A sophisticated model combining LSTMs, attention, and gating mechanisms for interpretable forecasting\n",
    "\n",
    "2. **Data Handling**: TFT gracefully handles three types of inputs:\n",
    "   - Static features (time-invariant)\n",
    "   - Known future features (available at forecast time)\n",
    "   - Observed features (historical only)\n",
    "\n",
    "3. **Probabilistic Forecasting**: Quantile predictions provide uncertainty estimates, not just point forecasts\n",
    "\n",
    "4. **Interpretability**: Built-in mechanisms reveal:\n",
    "   - Which historical time steps influence predictions (attention)\n",
    "   - Which features are most important (variable selection)\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "| Aspect | TFT Advantage |\n",
    "|--------|---------------|\n",
    "| **Accuracy** | State-of-the-art forecasting performance |\n",
    "| **Uncertainty** | Quantile outputs for confidence intervals |\n",
    "| **Interpretability** | Attention + variable importance |\n",
    "| **Flexibility** | Handles mixed input types |\n",
    "| **Scalability** | Works with multiple time series |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"TFT TUTORIAL COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  Final Training Loss:   {trainer.history['train_loss'][-1]:.6f}\")\n",
    "print(f\"  Final Validation Loss: {trainer.history['val_loss'][-1]:.6f}\")\n",
    "print(f\"  Best Validation Loss:  {trainer.best_val_loss:.6f}\")\n",
    "print(f\"  Test Loss:             {test_metrics['loss']:.6f}\")\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Parameters:      {total_params:,}\")\n",
    "print(f\"  Encoder Length:  {config.encoder_length}\")\n",
    "print(f\"  Decoder Length:  {config.decoder_length}\")\n",
    "print(f\"  Hidden Size:     {config.hidden_size}\")\n",
    "print(f\"\\nFiles Saved:\")\n",
    "print(f\"  Model Checkpoint: tft_best_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
